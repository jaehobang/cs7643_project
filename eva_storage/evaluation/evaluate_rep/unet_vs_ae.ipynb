{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eva_vs_uniform sampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jbang36/eva_jaeho/eva_storage/evaluation/evaluate_rep'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.argv=['']\n",
    "sys.path.append('../../../')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#import utils.helpers as helpers\n",
    "import utils as helpers\n",
    "from loaders.uadetrac_loader import UADetracLoader\n",
    "from eva_storage.preprocessingModule import PreprocessingModule\n",
    "from eva_storage.UNet import UNet\n",
    "from eva_storage.clusterModule import ClusterModule\n",
    "from filters.minimum_filter import FilterMinimum\n",
    "from eva_storage.evaluation.evaluate_compression import *\n",
    "from eva_storage.models.Autoencoder import Autoencoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "#torch.cuda.set_device(0) # we want to use both gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UADetracLoader()\n",
    "images = loader.load_cached_images()\n",
    "video_start_indices = loader.get_video_start_indices()\n",
    "labels = loader.load_cached_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## done loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## assume we already have preprocessed the images + trained the network\n",
    "## if not, look at eva_vs_nop__ml.ipynb to find out how to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network\n",
    "ae = Autoencoder().cuda()\n",
    "loss_func = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataset object \n",
    "images_normalized = images.astype(np.float32)\n",
    "images_normalized /= 255.0\n",
    "train_data = torch.from_numpy(images_normalized)\n",
    "train_data = train_data.permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_dataset = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1/40 \tLoss: 0.014788\n",
      "Train Epoch: 2/40 \tLoss: 0.007426\n",
      "Train Epoch: 3/40 \tLoss: 0.006745\n",
      "Train Epoch: 4/40 \tLoss: 0.006359\n",
      "Train Epoch: 5/40 \tLoss: 0.006019\n",
      "Train Epoch: 6/40 \tLoss: 0.005699\n",
      "Train Epoch: 7/40 \tLoss: 0.005427\n",
      "Train Epoch: 8/40 \tLoss: 0.005208\n",
      "Train Epoch: 9/40 \tLoss: 0.005021\n",
      "Train Epoch: 10/40 \tLoss: 0.004869\n",
      "Train Epoch: 11/40 \tLoss: 0.004726\n",
      "Train Epoch: 12/40 \tLoss: 0.004616\n",
      "Train Epoch: 13/40 \tLoss: 0.004516\n",
      "Train Epoch: 14/40 \tLoss: 0.004410\n",
      "Train Epoch: 15/40 \tLoss: 0.004317\n",
      "Train Epoch: 16/40 \tLoss: 0.004236\n",
      "Train Epoch: 17/40 \tLoss: 0.004168\n",
      "Train Epoch: 18/40 \tLoss: 0.004089\n",
      "Train Epoch: 19/40 \tLoss: 0.004027\n",
      "Train Epoch: 20/40 \tLoss: 0.003973\n",
      "Train Epoch: 21/40 \tLoss: 0.003927\n",
      "Train Epoch: 22/40 \tLoss: 0.003876\n",
      "Train Epoch: 23/40 \tLoss: 0.003829\n",
      "Train Epoch: 24/40 \tLoss: 0.003788\n",
      "Train Epoch: 25/40 \tLoss: 0.003750\n",
      "Train Epoch: 26/40 \tLoss: 0.003709\n",
      "Train Epoch: 27/40 \tLoss: 0.004204\n",
      "Train Epoch: 28/40 \tLoss: 0.003655\n",
      "Train Epoch: 29/40 \tLoss: 0.003613\n",
      "Train Epoch: 30/40 \tLoss: 0.003579\n",
      "Train Epoch: 31/40 \tLoss: 0.003557\n",
      "Train Epoch: 32/40 \tLoss: 0.003506\n",
      "Train Epoch: 33/40 \tLoss: 0.003456\n",
      "Train Epoch: 34/40 \tLoss: 0.003426\n",
      "Train Epoch: 35/40 \tLoss: 0.003395\n",
      "Train Epoch: 36/40 \tLoss: 0.003368\n",
      "Train Epoch: 37/40 \tLoss: 0.003342\n",
      "Train Epoch: 38/40 \tLoss: 0.003322\n",
      "Train Epoch: 39/40 \tLoss: 0.003306\n",
      "Train Epoch: 40/40 \tLoss: 0.003284\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "EPOCHS = 40\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, data in enumerate(image_dataset):   \n",
    "        \n",
    "        \n",
    "        data = data.cuda()\n",
    "        #print(data.size())\n",
    "        compressed, final = ae(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(final, data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    print('Train Epoch: {}/{} \\tLoss: {:.6f}'.format(\n",
    "        epoch+1,\n",
    "        EPOCHS, \n",
    "        loss.cuda().data.item()), \n",
    "        end='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the trained model as.... /home/jbang36/eva_jaeho/models/ae.pth\n"
     ]
    }
   ],
   "source": [
    "### saving the trained network\n",
    "save_directory = '/home/jbang36/eva_jaeho/models/ae.pth'\n",
    "print(\"Saving the trained model as....\", save_directory)\n",
    "\n",
    "torch.save(ae.state_dict(), save_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "### loading the trained network\n",
    "save_directory = '/home/jbang36/eva_jaeho/models/ae.pth'  \n",
    "ae = Autoencoder()\n",
    "ae.load_state_dict(torch.load(save_directory))\n",
    "ae = ae.cuda()\n",
    "print(\"Model successfully loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_data = np.ndarray(shape=(train_data.shape[0], 9, 75, 75)) \n",
    "for i, images in enumerate(image_dataset):\n",
    "    images = images.cuda()\n",
    "    compressed, final = ae(images)\n",
    "    compressed_cpu = compressed.detach().cpu().numpy()\n",
    "    compressed_cpu *= 255\n",
    "    compressed_cpu = compressed_cpu.astype(np.uint8)\n",
    "    compressed_data[i * batch_size:(i + 1) * batch_size] = compressed_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25435, 9, 75, 75)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## too lazy to change variables....\n",
    "unet_compressed_images = compressed_data\n",
    "\n",
    "#need to flatten the data\n",
    "unet_compressed_images = unet_compressed_images.reshape(train_data.shape[0], 9*75*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25435, 50625)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_compressed_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 5]\n",
      " [1 2 3 4 5]\n",
      " [1 2 3 4 5]\n",
      " [1 2 3 4 5]\n",
      " [1 2 3 4 5]]\n",
      "-------------\n",
      "[[1 3 5]\n",
      " [1 3 5]\n",
      " [1 3 5]\n",
      " [1 3 5]\n",
      " [1 3 5]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5],[1,2,3,4,5]])\n",
    "b = a[:,::2]\n",
    "print(a)\n",
    "print(\"-------------\")\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25435, 225)\n"
     ]
    }
   ],
   "source": [
    "## we want to downsample this because 50625 features is way too big\n",
    "## how about something like 9 * 5 * 5??\n",
    "unet_compressed_images = compressed_data[:,::25,::25]\n",
    "unet_compressed_images = unet_compressed_images.reshape(train_data.shape[0], -1)\n",
    "print(unet_compressed_images.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to fit  25435 :  90.66402435302734\n"
     ]
    }
   ],
   "source": [
    "# Create clusters\n",
    "cm = ClusterModule()\n",
    "image_cluster_labels = cm.run(unet_compressed_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary labels\n",
    "## within labels['vehicle'] there are ['car', 'others', 'van', 'bus']\n",
    "\n",
    "car_labels = helpers.generateBinaryLabels(labels['vehicle'])\n",
    "other_labels = helpers.generateBinaryLabels(labels['vehicle'], label_of_interest = 'others')\n",
    "van_labels = helpers.generateBinaryLabels(labels['vehicle'], 'van')\n",
    "bus_labels = helpers.generateBinaryLabels(labels['vehicle'], 'bus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the training and validation dataset for all frames\n",
    "division_point = int(unet_compressed_images.shape[0] * 0.8)\n",
    "train_set = {}\n",
    "test_set = {}\n",
    "    \n",
    "# this set contains [image, labels, cluster_labels]\n",
    "train_set['van'] = [train_data[:division_point], van_labels[:division_point], image_cluster_labels[:division_point]]\n",
    "train_set['bus'] = [train_data[:division_point], bus_labels[:division_point], image_cluster_labels[:division_point]]\n",
    "test_set['van'] = [train_data[division_point:], van_labels[division_point:], image_cluster_labels[division_point:]]\n",
    "test_set['bus'] = [train_data[division_point:], bus_labels[division_point:], image_cluster_labels[division_point:]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20348\n",
      "torch.Size([25435, 3, 300, 300])\n",
      "(25435, 225)\n"
     ]
    }
   ],
   "source": [
    "print(division_point)\n",
    "print(train_data.shape)\n",
    "print(unet_compressed_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25435, 3, 300, 300])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the training and validation dataset for rep frames\n",
    "#division_point = int(rep_images.shape[0] * 0.8)\n",
    "\n",
    "rep_train_set = {}\n",
    "rep_test_set = {}\n",
    "for key in ['van', 'bus']:\n",
    "    rep_train_set[key] = get_rep_frames(train_set[key][0], train_set[key][1], train_set[key][2])\n",
    "    rep_test_set[key] = get_rep_frames(test_set[key][0], test_set[key][1], test_set[key][2])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training van .....\n",
      "torch.Size([16278, 270000])\n",
      "(16278,)\n",
      "(1016, 270000)\n",
      "(1016,)\n",
      "training bus .....\n",
      "torch.Size([16278, 270000])\n",
      "(16278,)\n",
      "(1016, 270000)\n",
      "(1016,)\n"
     ]
    }
   ],
   "source": [
    "# Test on filters\n",
    "fm_everyframe_set = {}\n",
    "fm_repframe_set = {}\n",
    "for key in ['van', 'bus']:\n",
    "    print(\"training\", key, \".....\")\n",
    "    fm_everyframe_set[key] = FilterMinimum()\n",
    "    fm_everyframe_set[key].train(train_set[key][0], train_set[key][1])\n",
    "    \n",
    "    fm_repframe_set[key] = FilterMinimum()\n",
    "    fm_repframe_set[key].train(rep_train_set[key][0], rep_train_set[key][1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the filters\n",
    "# add y_hat as the third element in the test_set\n",
    "# test set [images, labels, cluster labels, predicted labels]\n",
    "\n",
    "for key in ['van', 'bus']:\n",
    "    test_set[key].append(fm_everyframe_set[key].predict(test_set[key][0], post_model_name= 'rf'))\n",
    "    rep_test_set[key] = list(rep_test_set[key])\n",
    "    rep_test_set[key].append(fm_repframe_set[key].predict(rep_test_set[key][0], post_model_name = 'rf'))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van score is 43.876548063691764 %\n",
      "bus score is 59.68154118340869 %\n"
     ]
    }
   ],
   "source": [
    "## compute all frame score\n",
    "\n",
    "for key in ['van', 'bus']:\n",
    "    tmp = np.array(test_set[key][1] - test_set[key][3])\n",
    "    all_frame_score = len(tmp[tmp == 0]) / float(len(tmp))\n",
    "    print(key, \"score is\", all_frame_score * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## compute rep frame score\n",
    "# this requires me to assess -> spread the labels -> compare against original results\n",
    "\n",
    "def extend_labels(y_hat_rep, X_test_clusters):\n",
    "    arr = np.zeros(shape = (len(X_test_clusters)))\n",
    "    \n",
    "    for i in range(len(X_test_clusters)):\n",
    "        arr[i] = y_hat_rep[X_test_clusters[i]]\n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van score is 44.44662866129349 %\n",
      "bus score is 48.67308826420287 %\n"
     ]
    }
   ],
   "source": [
    "for key in ['van', 'bus']:\n",
    "    y_hat = extend_labels(rep_test_set[key][2], test_set[key][2])\n",
    "    tmp = np.array(y_hat - test_set[key][1])\n",
    "    rep_frame_score = len(tmp[tmp == 0]) / float(len(tmp))\n",
    "    print(key, \"score is\", rep_frame_score * 100, \"%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniform sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need logic to perform comparison vs uniform sampling\n",
    "\n",
    "fps = 20\n",
    "uniform_indexes_train = [i for i in range(0, division_point, fps)]\n",
    "uniform_indexes_test = [(i - division_point) for i in range(division_point, end_frame, fps)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25435, 225)\n"
     ]
    }
   ],
   "source": [
    "print(unet_compressed_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25435\n"
     ]
    }
   ],
   "source": [
    "print(end_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1018\n",
      "255\n"
     ]
    }
   ],
   "source": [
    "print(len(uniform_indexes_train))\n",
    "print(len(uniform_indexes_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## steps\n",
    "## 1. get the corresponding labels, train the models, evaluate the model, and extend the labels\n",
    "\n",
    "## this set contains [image, labels, cluster_labels]\n",
    "## divide the training and validation dataset for rep frames\n",
    "## division_point = int(rep_images.shape[0] * 0.8)\n",
    "\n",
    "sample_train_set = {}\n",
    "sample_test_set = {}\n",
    "for key in ['van', 'bus']:\n",
    "    sample_train_set[key] = (train_set[key][0][uniform_indexes_train], train_set[key][1][uniform_indexes_train]) ##rep_images, rep_labels get_rep_frames(train_set[key][0], train_set[key][1], train_set[key][2])\n",
    "    sample_test_set[key] = (test_set[key][0][uniform_indexes_test], test_set[key][1][uniform_indexes_test])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1018, 3, 300, 300])\n",
      "torch.Size([255, 3, 300, 300])\n"
     ]
    }
   ],
   "source": [
    "print(sample_train_set['van'][0].shape)\n",
    "print(sample_test_set['van'][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training van .....\n",
      "torch.Size([814, 270000])\n",
      "(814,)\n",
      "training bus .....\n",
      "torch.Size([814, 270000])\n",
      "(814,)\n"
     ]
    }
   ],
   "source": [
    "# Test on filters\n",
    "fm_uniframe_set = {}\n",
    "for key in ['van', 'bus']:\n",
    "    print(\"training\", key, \".....\")\n",
    "    fm_uniframe_set[key] = FilterMinimum()\n",
    "    fm_uniframe_set[key].train(sample_train_set[key][0], sample_train_set[key][1])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the filters\n",
    "# add y_hat as the third element in the test_set\n",
    "# test set [images, labels, cluster labels, predicted labels]\n",
    "\n",
    "for key in ['van', 'bus']:\n",
    "    #test_set[key].append(fm_everyframe_set[key].predict(test_set[key][0], post_model_name= 'rf'))\n",
    "    sample_test_set[key] = list(sample_test_set[key])\n",
    "    sample_test_set[key].append(fm_uniframe_set[key].predict(sample_test_set[key][0], post_model_name = 'rf'))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van score is 43.876548063691764 %\n",
      "bus score is 59.68154118340869 %\n"
     ]
    }
   ],
   "source": [
    "## compute all frame score\n",
    "\n",
    "for key in ['van', 'bus']:\n",
    "    tmp = np.array(test_set[key][1] - test_set[key][3])\n",
    "    all_frame_score = len(tmp[tmp == 0]) / float(len(tmp))\n",
    "    print(key, \"score is\", all_frame_score * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to extend the labels\n",
    "## TODO: modify this function for sampling\n",
    "## for sampling, we will assume each 'cluster' is \n",
    "## from the starting frame to the frame right before the end\n",
    "\n",
    "\n",
    "def extend_labels_sampling(y_hat_samp, images):\n",
    "    end_frame = len(images)\n",
    "    fps = 20\n",
    "    arr = np.zeros(shape = (len(images)))\n",
    "    print(arr.shape)\n",
    "    print(len(y_hat_samp))\n",
    "    \n",
    "    j = -1\n",
    "    for i in range(len(images)):\n",
    "        if i % fps == 0:\n",
    "            j += 1\n",
    "        arr[i] = y_hat_samp[j]\n",
    "    \n",
    "    return arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5087,)\n",
      "255\n",
      "van score is 60.19264792608611 %\n",
      "(5087,)\n",
      "255\n",
      "bus score is 46.03892274425005 %\n"
     ]
    }
   ],
   "source": [
    "for key in ['van', 'bus']:\n",
    "    y_hat = extend_labels_sampling(sample_test_set[key][2], unet_compressed_images[division_point:])\n",
    "    tmp = np.array(y_hat - test_set[key][1])\n",
    "    rep_frame_score = len(tmp[tmp == 0]) / float(len(tmp))\n",
    "    print(key, \"score is\", rep_frame_score * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the distribution of the data??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "van positive percentage is 57.89266758403774 %\n",
      "bus positive percentage is 65.5396107725575 %\n"
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "train_set['van']\n",
    "train_set['bus']\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "for key in ['van', 'bus']:\n",
    "    label = test_set[key][1]\n",
    "    positive = sum(label) * 1.0 / len(label)\n",
    "    print(key, \"positive percentage is\", positive * 100, \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
